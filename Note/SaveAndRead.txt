Reading
=======
Using Scala
-----------
val rdd = sc.TextFile(<Path>)

val parquet_df = sqlContext.read.parquet("<PATH>")

val orc_df = sqlContext.read.orc(<PATH>)

import com.databricks.spark.avro._;
val avro_Df = sqlContext.read.avro(<PATH>);

val json_df = sqlContext.read.json("<PATH>")
val json_df = sqlContext.read.format("json").load("<PATH>")



Using Python
-------------
rdd = sc.TextFile(<Path>)

parquet_df = sqlContext.read.parquet("<PATH>")

orc_df = hiveContext.read.orc(<PATH>)

--packages com.databricks:spark-avro_2.10:2.0.1
avro_df = spark.read.format("com.databricks.spark.avro").load("<PATH>")

json_df = sqlContext.read.json("<PATH>"
json_df = sqlContext.read.load("<PATH>", format="json")

Writing
=======
Using Scala
-----------
rdd.saveAsTextFile(<Path>)
rdd.saveAsTextFile(<Path>, org.apache.hadoop.io.compress.GzipCodec) // BZip2Codec or GZipCodec or SnappyCodec

parquet_df.write.format("parquet").save("<PATH>") # Default: Snappy
sqlContext.setConf("spark.sql.parquet.compression.codec","gzip") // none,gzip,lzo,snappy,uncompressed

orc_df.write.mode(SaveMode.Overwrite).format("orc").save("<PATH>")

sqlContext.setConf("spark.sql.avro.compression.codec","snappy") //use snappy, deflate, uncompressed;
dataFrame.write.avro(<path to location>);

json_df.write.mode('append').json("<PATH>")
json_df.write.format("org.apache.spark.sql.json").mode(SaveMode.Append).save(<PATH>)

Using Python
------------
rdd.saveAsTextFile(<Path>)
rdd.saveAsTextFile(<Path>, org.apache.hadoop.io.compress.GzipCodec) # BZip2Codec or GZipCodec or SnappyCodec

parquet_df.write.save("<PATH>", format="parquet")
parquet_df.write.format("parquet").save("<PATH>") # Default: Snappy
parquet_df.write.option("compression", "gzip").mode("overwrite").save("<PATH>") # none,gzip,lzo,snappy,uncompressed

orc_df.write.format("orc").save("<PATH>")
orc_df.write.format("orc").option("compression", "zlib").mode("overwrite").save("<PATH>") # uncompressed, lzo, snappy, zlib, none

sqlContext.setConf("spark.sql.avro.compression.codec", "deflate") # uncompressed, snappy and deflate
avro_df.write.format("com.databricks.spark.avro").mode("overwrite").save("<PATH>")

json_df.write.mode('append').json("<PATH>")
json_df.write.format("org.apache.spark.sql.json").mode("append").save(<PATH>)

